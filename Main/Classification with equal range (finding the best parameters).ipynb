{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Genotyped.csv\", index_col='index')\n",
    "output = pd.read_csv(\"Phenotypes.csv\", index_col=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 5\n",
    "num_bins = 3\n",
    "split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599, 1279)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_phen'] = output['average phenotypes']\n",
    "df_sort = df.sort_values('avg_phen', ascending=False)\n",
    "df_sort = df_sort.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7967644880000002, -2.339534051)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest = df_sort.iloc[0, -1:][0]\n",
    "lowest = df_sort.iloc[-1, -1:][0]\n",
    "highest, lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8272597078"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = (highest - lowest) / num_steps\n",
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "border = highest\n",
    "df_final = df_sort.iloc[:, :-1]\n",
    "\n",
    "for i in range(num_bins):\n",
    "    df_final.loc[df_sort['avg_phen'] <= border, 'bin_n'] = i\n",
    "    border = border - step\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599, 1279)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_base = df_final.drop(columns=['bin_n'])\n",
    "X_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599,)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_base = df_final['bin_n']\n",
    "y_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectNFeatures(X, y, n):\n",
    "    skb = SelectKBest(f_regression, k=n)\n",
    "    skb.fit(X, y)\n",
    "    print(\"\\nSample with \", n, \" best paramenets\")\n",
    "    return skb.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doSMOTE(X, y):\n",
    "    smote = SMOTE()\n",
    "    X_after, y_after = smote.fit_resample(X, y)\n",
    "    print(\"\\nSMOTE applied\")\n",
    "    print(\"Before SMOTE: \", Counter(y))\n",
    "    print(\"After SMOTE: \", Counter(y_after), '\\n')\n",
    "    return X_after, y_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(X, y, X_train, X_test, y_train, y_test):\n",
    "    parameters = {\n",
    "        'n_estimators': [10,50,100],\n",
    "        'max_depth' : [4,5,7,10],\n",
    "        'learning_rate' : [0.0001, 0.001, 0.01]\n",
    "    }\n",
    "    classifier = xgboost.XGBClassifier()\n",
    "    random_search = RandomizedSearchCV(\n",
    "        classifier,\n",
    "        param_distributions=parameters,\n",
    "        n_iter=5,\n",
    "        scoring='f1_micro',\n",
    "        n_jobs=-1,\n",
    "        cv=5,\n",
    "        verbose=3,\n",
    "    )\n",
    "    random_search.fit(X, y)\n",
    "    best_estimator = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    print (\"\\nBest parameters: \", best_estimator, \"\\n\")\n",
    "    \n",
    "    best_clf = xgboost.XGBClassifier(objective='reg:squarederror',\n",
    "                        n_estimators=best_params['n_estimators'], \n",
    "                        max_depth=best_params['max_depth'], \n",
    "                        learning_rate=best_params['learning_rate'])\n",
    "    \n",
    "    best_clf.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = best_clf.predict(X_test)\n",
    "    y_pred_tr = best_clf.predict(X_train)\n",
    "    \n",
    "    return y_pred, y_pred_tr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_train, y_pred_tr, y_test, y_pred):\n",
    "    print('Training set:',f1_score(y_train,y_pred_tr, average='macro'))\n",
    "    print('Test set:',f1_score(y_test,y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionMatrix(y_test, y_pred):\n",
    "    print('\\n\\n', pd.crosstab(y_test, y_pred), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample with  100  best paramenets\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:    4.7s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:  XGBClassifier(learning_rate=0.01, max_depth=7, n_estimators=50,\n",
      "              objective='multi:softprob') \n",
      "\n",
      "Evaluation for n= 100 . Without SMOTE\n",
      "Training set: 0.7665113681491853\n",
      "Test set: 0.42235092235092236\n",
      "\n",
      "\n",
      " col_0  0.0  1.0  2.0\n",
      "bin_n               \n",
      "0.0      0    5    2\n",
      "1.0      0   22   25\n",
      "2.0      1   10   55 \n",
      "\n",
      "\n",
      "\n",
      "SMOTE applied\n",
      "Before SMOTE:  Counter({2.0: 342, 1.0: 218, 0.0: 39})\n",
      "After SMOTE:  Counter({0.0: 342, 1.0: 342, 2.0: 342}) \n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:    1.6s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:  XGBClassifier(learning_rate=0.001, max_depth=10, n_estimators=50,\n",
      "              objective='multi:softprob') \n",
      "\n",
      "Evaluation for n= 100 . With SMOTE\n",
      "Training set: 0.9106815933529466\n",
      "Test set: 0.7788387323271043\n",
      "\n",
      "\n",
      " col_0  0.0  1.0  2.0\n",
      "bin_n               \n",
      "0.0     60    6    1\n",
      "1.0     10   44    8\n",
      "2.0      3   17   57 \n",
      "\n",
      "\n",
      "\n",
      "Sample with  200  best paramenets\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:    5.1s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    5.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:  XGBClassifier(learning_rate=0.01, max_depth=4, objective='multi:softprob') \n",
      "\n",
      "Evaluation for n= 200 . Without SMOTE\n",
      "Training set: 0.7050754613290163\n",
      "Test set: 0.38368942885071916\n",
      "\n",
      "\n",
      " col_0  0.0  1.0  2.0\n",
      "bin_n               \n",
      "0.0      0    5    2\n",
      "1.0      1   16   30\n",
      "2.0      0    9   57 \n",
      "\n",
      "\n",
      "\n",
      "SMOTE applied\n",
      "Before SMOTE:  Counter({2.0: 342, 1.0: 218, 0.0: 39})\n",
      "After SMOTE:  Counter({0.0: 342, 1.0: 342, 2.0: 342}) \n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:    3.0s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:  XGBClassifier(learning_rate=0.01, max_depth=5, objective='multi:softprob') \n",
      "\n",
      "Evaluation for n= 200 . With SMOTE\n",
      "Training set: 0.8671365116184079\n",
      "Test set: 0.6927755819060167\n",
      "\n",
      "\n",
      " col_0  0.0  1.0  2.0\n",
      "bin_n               \n",
      "0.0     63    3    1\n",
      "1.0     18   33   11\n",
      "2.0      6   22   49 \n",
      "\n",
      "\n",
      "\n",
      "Sample with  300  best paramenets\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:    1.4s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:  XGBClassifier(learning_rate=0.001, max_depth=7, n_estimators=10,\n",
      "              objective='multi:softprob') \n",
      "\n",
      "Evaluation for n= 300 . Without SMOTE\n",
      "Training set: 0.8608778352625666\n",
      "Test set: 0.3773337700622339\n",
      "\n",
      "\n",
      " col_0  0.0  1.0  2.0\n",
      "bin_n               \n",
      "0.0      0    4    3\n",
      "1.0      4   19   24\n",
      "2.0      1   16   49 \n",
      "\n",
      "\n",
      "\n",
      "SMOTE applied\n",
      "Before SMOTE:  Counter({2.0: 342, 1.0: 218, 0.0: 39})\n",
      "After SMOTE:  Counter({0.0: 342, 1.0: 342, 2.0: 342}) \n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   11.2s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   13.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:  XGBClassifier(learning_rate=0.01, max_depth=10, objective='multi:softprob') \n",
      "\n",
      "Evaluation for n= 300 . With SMOTE\n",
      "Training set: 0.962403458250957\n",
      "Test set: 0.7954525449249895\n",
      "\n",
      "\n",
      " col_0  0.0  1.0  2.0\n",
      "bin_n               \n",
      "0.0     65    2    0\n",
      "1.0     11   43    8\n",
      "2.0      4   16   57 \n",
      "\n",
      "\n",
      "\n",
      "Sample with  400  best paramenets\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:    3.9s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:  XGBClassifier(learning_rate=0.0001, max_depth=7, n_estimators=10,\n",
      "              objective='multi:softprob') \n",
      "\n",
      "Evaluation for n= 400 . Without SMOTE\n",
      "Training set: 0.8322789367675143\n",
      "Test set: 0.3481096681096681\n",
      "\n",
      "\n",
      " col_0  0.0  1.0  2.0\n",
      "bin_n               \n",
      "0.0      0    4    3\n",
      "1.0      6   13   28\n",
      "2.0      0   13   53 \n",
      "\n",
      "\n",
      "\n",
      "SMOTE applied\n",
      "Before SMOTE:  Counter({2.0: 342, 1.0: 218, 0.0: 39})\n",
      "After SMOTE:  Counter({0.0: 342, 1.0: 342, 2.0: 342}) \n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:    3.9s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    9.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:  XGBClassifier(learning_rate=0.0001, max_depth=7, n_estimators=50,\n",
      "              objective='multi:softprob') \n",
      "\n",
      "Evaluation for n= 400 . With SMOTE\n",
      "Training set: 0.9041458862245452\n",
      "Test set: 0.7057079525568734\n",
      "\n",
      "\n",
      " col_0  0.0  1.0  2.0\n",
      "bin_n               \n",
      "0.0     64    2    1\n",
      "1.0     12   36   14\n",
      "2.0      5   25   47 \n",
      "\n",
      "\n",
      "\n",
      "Sample with  500  best paramenets\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:    1.8s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:  XGBClassifier(learning_rate=0.001, max_depth=7, n_estimators=10,\n",
      "              objective='multi:softprob') \n",
      "\n",
      "Evaluation for n= 500 . Without SMOTE\n",
      "Training set: 0.8162946870050742\n",
      "Test set: 0.37693481578373667\n",
      "\n",
      "\n",
      " col_0  0.0  1.0  2.0\n",
      "bin_n               \n",
      "0.0      0    4    3\n",
      "1.0      4   20   23\n",
      "2.0      2   17   47 \n",
      "\n",
      "\n",
      "\n",
      "SMOTE applied\n",
      "Before SMOTE:  Counter({2.0: 342, 1.0: 218, 0.0: 39})\n",
      "After SMOTE:  Counter({0.0: 342, 1.0: 342, 2.0: 342}) \n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   16.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   18.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:  XGBClassifier(learning_rate=0.001, max_depth=10, n_estimators=50,\n",
      "              objective='multi:softprob') \n",
      "\n",
      "Evaluation for n= 500 . With SMOTE\n",
      "Training set: 0.9561826919689729\n",
      "Test set: 0.719678938487855\n",
      "\n",
      "\n",
      " col_0  0.0  1.0  2.0\n",
      "bin_n               \n",
      "0.0     65    1    1\n",
      "1.0      7   45   10\n",
      "2.0      3   35   39 \n",
      "\n",
      "\n",
      "\n",
      "Sample with  600  best paramenets\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:    1.5s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:  XGBClassifier(learning_rate=0.001, max_depth=4, n_estimators=10,\n",
      "              objective='multi:softprob') \n",
      "\n",
      "Evaluation for n= 600 . Without SMOTE\n",
      "Training set: 0.6592521661646086\n",
      "Test set: 0.3228308563340411\n",
      "\n",
      "\n",
      " col_0  0.0  1.0  2.0\n",
      "bin_n               \n",
      "0.0      0    4    3\n",
      "1.0      1   11   35\n",
      "2.0      0   13   53 \n",
      "\n",
      "\n",
      "\n",
      "SMOTE applied\n",
      "Before SMOTE:  Counter({2.0: 342, 1.0: 218, 0.0: 39})\n",
      "After SMOTE:  Counter({0.0: 342, 1.0: 342, 2.0: 342}) \n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   19.8s remaining:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   25.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:  XGBClassifier(learning_rate=0.01, max_depth=7, n_estimators=50,\n",
      "              objective='multi:softprob') \n",
      "\n",
      "Evaluation for n= 600 . With SMOTE\n",
      "Training set: 0.9436437708679701\n",
      "Test set: 0.7438226031793201\n",
      "\n",
      "\n",
      " col_0  0.0  1.0  2.0\n",
      "bin_n               \n",
      "0.0     65    1    1\n",
      "1.0      8   45    9\n",
      "2.0      4   29   44 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(100, 700, 100):\n",
    "    y = y_base\n",
    "    X = selectNFeatures(X_base, y, n)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=42)\n",
    "        \n",
    "    # Train the model without SMOTE\n",
    "    y_pred, y_pred_tr = trainModel(X, y, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Evaluating the model without SMOTE\n",
    "    print('Evaluation for n=', n, '. Without SMOTE')\n",
    "    evaluate(y_train, y_pred_tr, y_test, y_pred)\n",
    "    confusionMatrix(y_test, y_pred)\n",
    "    \n",
    "    X, y = doSMOTE(X, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=42)\n",
    "    \n",
    "    #training the model with SMOTE\n",
    "    y_pred, y_pred_tr = trainModel(X, y, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Evaluating the model with SMOTE\n",
    "    print('Evaluation for n=', n, '. With SMOTE')\n",
    "    evaluate(y_train, y_pred_tr, y_test, y_pred)\n",
    "    confusionMatrix(y_test, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
