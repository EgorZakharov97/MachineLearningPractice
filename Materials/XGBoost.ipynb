{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bias Varias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=EuBBz3bI-aA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say, a mice a weight of **w** and a hight of **h**. We would like to predict mice' hight given its weight. We dont have a mathematical formula that describes the relationship between mice' weight and hight. We are going to use machine leatning techniques to find this formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between mice' weight and hight is not a straight line. The first technique we are using is the **Linear Regression**. It fits a straight line to the training set. The straight line does not any flexibility to accurately replicate the arc in the true relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The inability for a machine learning learning method like Linear Regression to capture the true relationship is called BIAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Squiggly line** fit the training set by calculating their sums of squares and fits perfectly all of the training set points. But for testing set, this line can perform even worse that a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology:** Because the Squiggly line fits the training set really well but not the testing set we say that it is **Overfit**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine learning, the ideal algorithm has **low bias** and can accurately model the true relationship. And it has **low variability**, by producing different datasets. This is done by finding the sweet spot between a simple model and a complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology:** Three commonly used methods for finding the sweet spot between simple and complicated models are: **regularization**, **boosting**, and **bagging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decicion Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=7VeUPuFGJHk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For decicion trees we have to find the first leave in a tree. Because none of the leaf nodes can predict with 100% accuracy, they are called **impure**. To determine which separation is best, we need a way to measure and compare **impurity**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method explain to measure impurity is called **Gini**. The lower impurity, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AdaBoost (Gradient Boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=LsK-xG1cLYA&list=PLblh5JKOoLUIcdlgu78MnlATeyx4cEVeR&index=22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AdaBoost trees contain only two leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology:** A tree with one node and two leaves is called a **Stump**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stumps are not great at making accurate classifications, they are weak learners. There are many of these small trees. In random forest, all trees has equal weight of vote. In AdaBoost, there are trees that's votes are more reliable that others. The errors that the first stump makes influence how the second stump is made, and the errors that the second stump makes influence how the third stump is made, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crearly:\n",
    "1. AddBoost combines a lot of \"weak learners\" to make classifications. The week learners are almost always **stumps**.\n",
    "2. Some stumps get more say in the classification than others.\n",
    "3. Each stump is made by taking the previous stump's mistake into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross validation (Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " https://www.youtube.com/watch?v=fSytzGwwBVw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The way to understand which model to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes different chunks of data and trains the algorithm. Then, tests it with the remaining chunk of data. The algorithm does that several times to understand which chunk is the best for testing and which for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to device the data into ten blocks. It is called **Ten-Fold Cross Validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is designed to work with large and complicated sets of data. It works similar to the Gradient Boost, but uses a different king of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common way to construct a XGBoost tree is as follows:\n",
    "1. Each tree starts out as a single leaf and all of the Residuals go to the leaf.\n",
    "2. Calculate a **Quantity Score** or **Similarity Score**, for the **Residuals**.\n",
    "   1. Similarity Score = Sum of Residuals, Squared / Number of Residuals + Lambda\n",
    "   2. Lambda is a regularization parameter\n",
    "   3. Let it be 0 at the beginning\n",
    "3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
