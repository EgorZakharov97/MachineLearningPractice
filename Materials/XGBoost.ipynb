{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bias Varias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=EuBBz3bI-aA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say, a mice a weight of **w** and a hight of **h**. We would like to predict mice' hight given its weight. We dont have a mathematical formula that describes the relationship between mice' weight and hight. We are going to use machine leatning techniques to find this formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between mice' weight and hight is not a straight line. The first technique we are using is the **Linear Regression**. It fits a straight line to the training set. The straight line does not any flexibility to accurately replicate the arc in the true relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The inability for a machine learning learning method like Linear Regression to capture the true relationship is called BIAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Squiggly line** fit the training set by calculating their sums of squares and fits perfectly all of the training set points. But for testing set, this line can perform even worse that a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology:** Because the Squiggly line fits the training set really well but not the testing set we say that it is **Overfit**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine learning, the ideal algorithm has **low bias** and can accurately model the true relationship. And it has **low variability**, by producing different datasets. This is done by finding the sweet spot between a simple model and a complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology:** Three commonly used methods for finding the sweet spot between simple and complicated models are: **regularization**, **boosting**, and **bagging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decicion Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=7VeUPuFGJHk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For decicion trees we have to find the first leave in a tree. Because none of the leaf nodes can predict with 100% accuracy, they are called **impure**. To determine which separation is best, we need a way to measure and compare **impurity**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method explain to measure impurity is called **Gini**. The lower impurity, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AdaBoost (Gradient Boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=LsK-xG1cLYA&list=PLblh5JKOoLUIcdlgu78MnlATeyx4cEVeR&index=22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AdaBoost trees contain only two leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology:** A tree with one node and two leaves is called a **Stump**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stumps are not great at making accurate classifications, they are weak learners. There are many of these small trees. In random forest, all trees has equal weight of vote. In AdaBoost, there are trees that's votes are more reliable that others. The errors that the first stump makes influence how the second stump is made, and the errors that the second stump makes influence how the third stump is made, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crearly:\n",
    "1. AddBoost combines a lot of \"weak learners\" to make classifications. The week learners are almost always **stumps**.\n",
    "2. Some stumps get more say in the classification than others.\n",
    "3. Each stump is made by taking the previous stump's mistake into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " https://www.youtube.com/watch?v=fSytzGwwBVw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross validation** allows to compare different machine learning methods and get a sence of how well they will work in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to do two things with the data:\n",
    "1. Estimate the parameters for the machine learning methods **Train the algorithm**.\n",
    "2. Evaluate how well the machine learning methods work. **Test the algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes different chunks of data and trains the algorithm. Then, tests it with the remaining chunk of data. The algorithm does that several times to understand which chunk is the best for testing and which for training. The nwe can compare the performance of each algorithm to determine the most suitable one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to device the data into ten blocks. It is called **Ten-Fold Cross Validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regularization (Ridge Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=Q81RR3yKn30&t=57s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use regerssion to find a line that fits the training data, it may not perfectly fit the test data. The idea is that we intrudice a small amount of Bias into how the New Line is fit to the data. By doint this, **Ridge Regression** can provide better long term predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression has a parameter **Lambda**. It can be anything from 0 to infinity.\n",
    "* When 0 => Ridge Regression penalty = 0; => Ridge Regression line will only minimize the Sum of Squared Residuals. Ridge regression line will be the same as the least squares line.\n",
    "* When >0 => (In case of prediction on Size based on Weight) Our prediction on size becomes less and less sensitive to weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What value to give Lambda?\n",
    "* Use Cross Validation (Ten-Fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thing to note\n",
    "If we have n features, we need at least n+1 data points to estimate parameters. If there is not enough data to find the Least Squares parameter estimates, Ridge Regression still can Cross Validation and the Ridge Regression Penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=OtD8wVaFm6E&t=395s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is designed to work with large and complicated sets of data. It works similar to the Gradient Boost, but uses a different king of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common way to construct a XGBoost tree is as follows:\n",
    "1. Each tree starts out as a single leaf and all of the Residuals go to the leaf.\n",
    "2. Calculate a **Quantity Score** or **Similarity Score**, for the **Residuals**.\n",
    "    1. Similarity Score = Sum of Residuals, Squared / Number of Residuals + Lambda.\n",
    "    2. Lambda is a regularization parameter.\n",
    "3. Split the residuals into two groups.\n",
    "    1. Focus on two observations with lowest values.\n",
    "    2. Find their average.\n",
    "    3. Split the main leaf into two groups according to the average.\n",
    "4. calculate the Similaruty Score again. It will be higher on a lower leafs.\n",
    "5. Quantify how much better the leaves cluster similar Residuals than the root.\n",
    "    1. By calculating the **Gain** of splitting Residuals into two groups.\n",
    "        1. Gain = Similarity(Left) + Similarity(Right) - Similarity(Root).\n",
    "    2. Compare it to Gain calculated for other thresholds and build a simple tree that divides the observations using the new threshold, Dosage < 22.5. \n",
    "    3. Calculate the Similarity scores and then Gain for the leafs. The bigger the Gain, the better is the value in terms of splitting the Residuals into cluster of similar values.\n",
    "6. Shift the threshold over so that it is the average of the last two observations.\n",
    "7. Build a simple tree that divides that observations using the new threshold.\n",
    "8. Repeat the process for the rest of the data.\n",
    "9. Use the threshold that gives the largest Gain for the first branch in the tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
